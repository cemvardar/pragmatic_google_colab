{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM71KzyxVji+LFveuNfKhzv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cemvardar/pragmatic_google_colab/blob/main/dslab_colab_utility_chatgpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install OpenAI\n",
        "!pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "from time import sleep\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "import concurrent.futures\n",
        "from pydantic import BaseModel"
      ],
      "metadata": {
        "id": "EJbav48Yzpxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b6735de-7fe2-4563-c064-f791bab30434"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.0/457.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.9/164.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_gpt_response_pydantic(system_content, user_content, output_object):\n",
        "    client = openai.OpenAI(api_key=userdata.get('openai_api_key'))\n",
        "    completion = client.beta.chat.completions.parse(\n",
        "        model=\"gpt-4o-2024-08-06\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_content},\n",
        "            {\"role\": \"user\", \"content\": user_content},\n",
        "        ],\n",
        "        response_format=output_object,\n",
        "    )\n",
        "    return completion.choices[0].message.parsed\n",
        "\n",
        "class RandomNumbers(BaseModel):\n",
        "    numbers: list[int]\n",
        "\n",
        "# system_content = \"you are a random number geenrator\"\n",
        "# user_content = \"generate 10 random numbers\"\n",
        "\n",
        "# response = generate_gpt_response_pydantic(system_content, user_content, RandomNumbers)\n",
        "# response"
      ],
      "metadata": {
        "id": "_BgqDkP_kz4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d32109-14d5-4ef2-acb6-f721ce22be7f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomNumbers(numbers=[42, 63, 16, 89, 29, 55, 73, 4, 37, 91])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_gpt_response_pydantic_parallel_worker(prompt_key, system_content, user_content, output_object, max_retries=3):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            return prompt_key, generate_gpt_response_pydantic(system_content, user_content, output_object)\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt+1} failed for {prompt_key}: {e}\")\n",
        "    return prompt_key, None  # Return None if all retries fail\n",
        "\n",
        "def generate_gpt_response_pydantic_parallel(prompts_key_system_user_object, max_retries=3):\n",
        "    responses = {}\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = [executor.submit(generate_gpt_response_pydantic_parallel_worker, key, system_content, user_content, output_object, max_retries)\n",
        "                      for key, system_content, user_content, output_object in prompts_key_system_user_object]\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            key, result = future.result()\n",
        "            responses[key] = result\n",
        "\n",
        "    return responses\n",
        "\n",
        "# prompts_key_system_user_object = []\n",
        "# for i in range(1, 15):\n",
        "#     system_content = f\"you are a random number generator {i}\"\n",
        "#     user_content = f\"generate {i} random numbers\"\n",
        "#     prompts_key_system_user_object.append((i, system_content, user_content, RandomNumbers))\n",
        "# responses = generate_gpt_response_pydantic_parallel(prompts_key_system_user_object)\n",
        "# responses"
      ],
      "metadata": {
        "id": "qQA7CxA8lvJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6azkF50Dzchk"
      },
      "outputs": [],
      "source": [
        "def check_run(client, thread_id, run_id):\n",
        "    while True:\n",
        "        # Refresh the run object to get the latest status\n",
        "        run = client.beta.threads.runs.retrieve(\n",
        "            thread_id=thread_id,\n",
        "            run_id=run_id\n",
        "        )\n",
        "\n",
        "        if run.status == \"completed\":\n",
        "            # print(f\"Run is completed.\")\n",
        "            break\n",
        "        elif run.status == \"expired\":\n",
        "            print(f\"Run is expired.\")\n",
        "            break\n",
        "        elif run.status == \"failed\":\n",
        "            print(f\"Run is failed.\")\n",
        "            break\n",
        "        else:\n",
        "            # print(f\"OpenAI: Run is not yet completed. Waiting...{run.status}\")\n",
        "            time.sleep(1)  # Wait for 1 second before checking again\n",
        "\n",
        "def get_file_name_for_export_with_date_time(file_name_header, file_extenstion):\n",
        "    formatted_datetime = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
        "    file_name = f\"{file_name_header}_{formatted_datetime}.{file_extenstion}\"\n",
        "    file_name = file_name.replace(' ', '_')\n",
        "    return file_name\n",
        "\n",
        "\n",
        "def last_chatgpt_reponse(messages):\n",
        "    return messages.data[0].content[0].text.value\n",
        "\n",
        "class ChatGpt_Assitant():\n",
        "    def __init__(self, name, instructions, tools, model):\n",
        "        self.client = openai.Client(api_key=userdata.get('openai_api_key'))\n",
        "        self.assistant = self.client.beta.assistants.create(\n",
        "            name=name,\n",
        "            instructions=instructions,\n",
        "            tools=tools,\n",
        "            model=model\n",
        "        )\n",
        "        self.thread = self.client.beta.threads.create()\n",
        "        self.messages = None\n",
        "\n",
        "    def add_user_input(self, user_input, print_repsonse=True):\n",
        "        message = self.client.beta.threads.messages.create(thread_id=self.thread.id,\n",
        "                                                    role=\"user\",\n",
        "                                                    content=user_input\n",
        "        )\n",
        "\n",
        "        run = self.client.beta.threads.runs.create(\n",
        "            thread_id=self.thread.id,\n",
        "            assistant_id=self.assistant.id,\n",
        "        )\n",
        "\n",
        "        check_run(self.client, self.thread.id, run.id)\n",
        "\n",
        "        self.messages = self.client.beta.threads.messages.list(\n",
        "            thread_id=self.thread.id\n",
        "        )\n",
        "\n",
        "\n",
        "        assistant_message = self.messages.data[0].content[0].text.value\n",
        "        if print_repsonse:\n",
        "            print(len(self.messages.data))\n",
        "            print(assistant_message)\n",
        "\n",
        "        return last_chatgpt_reponse(self.messages)\n",
        "\n",
        "\n",
        "def get_chatgpt_shopify_helper():\n",
        "    name = \"Shopify helper\"\n",
        "    instructions=\"\"\"You are a shopify site content generator for products and product collections.\n",
        "                    You generate  product titles and descriptions using the details provived by the users.\n",
        "                    you follow instructions and feedback from the users to generate the best titles and descriptions\n",
        "                    for maximizing product sales.\n",
        "                    \"\"\"\n",
        "    tools=[{\"type\": \"code_interpreter\"}]\n",
        "    model=\"gpt-4o\"\n",
        "    return ChatGpt_Assitant(name, instructions, tools, model)\n",
        "\n",
        "\n",
        "def parse_json_from_gpt_response(message):\n",
        "    start_index = message.find('```json') + 8\n",
        "    end_index = message.rfind(\"```\")\n",
        "    json_content = json.loads(message[start_index:end_index].replace('\\n\\n', ''))\n",
        "    return json_content\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_json_response_from_chat_gpt(prompt):\n",
        "    gpt_assistant = get_chatgpt_shopify_helper()\n",
        "    message = gpt_assistant.add_user_input(prompt, False)\n",
        "    try:\n",
        "        json_response = parse_json_from_gpt_response(message)\n",
        "        return json_response\n",
        "    except:\n",
        "        print('JSON error. Could not parse chatgpt response')\n",
        "        return message\n",
        "\n",
        "def get_image_description_json(image_url, prompt=None):\n",
        "    if prompt is None:\n",
        "        prompt = \"\"\"Describe what's in the photo. Respond with a JSON object. key will be image_description\"\"\"\n",
        "    message_content= [{ \"type\": \"text\",\n",
        "                         \"text\": prompt},\n",
        "                      {\"type\": \"image_url\",\n",
        "                       \"image_url\": {\"url\":image_url,\n",
        "                       \"detail\": \"high\"}}]\n",
        "    return get_json_response_from_chat_gpt(message_content)"
      ],
      "metadata": {
        "id": "4N_m3ZuUqgd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_json_response_for_prompt_parallel(prompt_key, prompt, max_retries=3):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            return prompt_key, get_json_response_from_chat_gpt(prompt)\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt+1} failed for {prompt_key}: {e}\")\n",
        "    return prompt_key, None  # Return None if all retries fail\n",
        "\n",
        "def get_json_response_for_image_parallel(prompt_key, image_url, prompt, max_retries=3):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            return prompt_key, get_image_description_json(image_url, prompt)\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt+1} failed for {prompt_key}: {e}\")\n",
        "    return prompt_key, None  # Return None if all retries fail\n",
        "\n",
        "def get_gpt_json_image_reponses_parallel(prompts_key_image_url_prompt, max_retries=3):\n",
        "    responses = {}\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = [executor.submit(get_json_response_for_image_parallel, key, image_url, prompt, max_retries)\n",
        "                      for key, image_url, prompt in prompts_key_image_url_prompt]\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            key, result = future.result()\n",
        "            responses[key] = result\n",
        "\n",
        "    return responses\n",
        "\n",
        "def get_gpt_json_reponses_parallel(prompts, max_retries=3):\n",
        "    responses = {}\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = [executor.submit(get_json_response_for_prompt_parallel, key, prompt, max_retries) for key, prompt in prompts]\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            key, result = future.result()\n",
        "            responses[key] = result\n",
        "\n",
        "    return responses\n"
      ],
      "metadata": {
        "id": "s5IvnaYmzzN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image_url = \"https://storage.googleapis.com/decision-science-lab-bucket/liplips/roma_silvers_products/rs2502/2581.jpg\"\n",
        "# prompt = \"\"\"\n",
        "# we'll be creating a new photo of the jewelery in the photo changing the bacground using an AI system.\n",
        "# Expand the following prompt to include different details about shows and lighting and appearaence to generate\n",
        "# a minimalistic and professional and cute looking photo.\n",
        "# ```prompt\n",
        "# on a white wood table with 1 dry flower\n",
        "# ```\n",
        "# Respond with a JSON object. key will be image_prompt\n",
        "# \"\"\"\n",
        "# # get_json_response_for_image_parallel(0, image_url, prompt)\n",
        "# image_prompts = [(0, image_url, prompt), (1, image_url, prompt), (2, image_url, None)]\n",
        "# get_gpt_json_image_reponses_parallel(image_prompts)"
      ],
      "metadata": {
        "id": "YVMSPMpRSKny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt_title= \"\"\"\n",
        "# Find a shopify product title and description including details about stone properties and\n",
        "# possible styles for the following product. title and description will be in english.\n",
        "# İnanna Lapis Kolye - Gold\n",
        "# {'*Ürünlerimiz el yapımıdır.', '*İnci ve Lapis taşı kullanılmıştır.', '925 Gümüş/Silver', '*Ürünlerimiz 925 ayar gümüştür.', '*Sarı Altın kaplama yapılmıştır.', \"*Kolye zincir uzunluğu 41 cm'dir. 5 cm uzatma zinciri vardır.\"}\n",
        "# '\n",
        "# Respond with a JSON object. keys will be title and description.\n",
        "# don't use column in text\n",
        "# \"\"\"\n",
        "# prompts = [('a', prompt_title), ('b', prompt_title), ('c', prompt_title)]\n",
        "# get_gpt_json_reponses_parallel(prompts)"
      ],
      "metadata": {
        "id": "qGSzitbO0Nwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gpt_assistant = get_chatgpt_shopify_helper()\n",
        "# gpt_assistant.add_user_input('Respond with a JSON object. Content for an example product', False)\n",
        "# message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2lBjQsO5PQv",
        "outputId": "42d04356-593a-4a08-a47d-9a3430172126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI: Run is not yet completed. Waiting...queued\n",
            "Run is completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt_title= \"\"\"\n",
        "# Find a shopify product title and description including details about stone properties and\n",
        "# possible styles for the following product. title and description will be in english.\n",
        "# İnanna Lapis Kolye - Gold\n",
        "# {'*Ürünlerimiz el yapımıdır.', '*İnci ve Lapis taşı kullanılmıştır.', '925 Gümüş/Silver', '*Ürünlerimiz 925 ayar gümüştür.', '*Sarı Altın kaplama yapılmıştır.', \"*Kolye zincir uzunluğu 41 cm'dir. 5 cm uzatma zinciri vardır.\"}\n",
        "# '\n",
        "# Respond with a JSON object. keys will be title and description.\n",
        "# don't use column in text\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "eCkkb7UWKqh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gpt_assistant = get_chatgpt_shopify_helper()\n",
        "# message = gpt_assistant.add_user_input(prompt_title, False)\n",
        "# print(message)\n",
        "# parse_json_from_gpt_response(message)"
      ],
      "metadata": {
        "id": "i82t3HZ0KryT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bp_prompt = '''\n",
        "# give 6 bullet point items in english for the following product\n",
        "# İnanna Lapis Kolye - Gold\n",
        "# {'*Ürünlerimiz el yapımıdır.', '*İnci ve Lapis taşı kullanılmıştır.', '925 Gümüş/Silver', '*Ürünlerimiz 925 ayar gümüştür.', '*Sarı Altın kaplama yapılmıştır.', \"*Kolye zincir uzunluğu 41 cm'dir. 5 cm uzatma zinciri vardır.\"}\n",
        "# Respond with a JSON object. key will be items\n",
        "# don't use column in text\n",
        "# '''\n",
        "# message = gpt_assistant.add_user_input(bp_prompt, False)\n",
        "# print(message)\n",
        "# parse_json_from_gpt_response(message)"
      ],
      "metadata": {
        "id": "VnUgkh4vNFZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image_url = \"https://storage.googleapis.com/decision-science-lab-bucket/liplips/roma_silvers_products/rs2502/2581.jpg\"\n",
        "# get_image_description_json(image_url)"
      ],
      "metadata": {
        "id": "tATZCw19s5ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image_url = \"https://storage.googleapis.com/decision-science-lab-bucket/liplips/roma_silvers_products/rs2502/2581.jpg\"\n",
        "# prompt = \"\"\"\n",
        "# we'll be creating a new photo of the jewelery in the photo changing the bacground using an AI system.\n",
        "# Expand the following prompt to include different details about shows and lighting and appearaence to generate\n",
        "# a minimalistic and professional and cute looking photo.\n",
        "# ```prompt\n",
        "# on a white wood table with 1 dry flower\n",
        "# ```\n",
        "# Respond with a JSON object. key will be image_prompt\n",
        "# \"\"\"\n",
        "# get_image_description_json(image_url, prompt)"
      ],
      "metadata": {
        "id": "DGKObiHjuDK0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}