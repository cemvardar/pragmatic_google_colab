{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjmif96ufhjRXgblT+ui9S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cemvardar/pragmatic_google_colab/blob/main/dslab_colab_utility_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h6YobZHn4CyP"
      },
      "outputs": [],
      "source": [
        "!pip install -q pymongo\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import random\n",
        "import folium\n",
        "from pymongo import MongoClient, UpdateOne\n",
        "import urllib.parse\n",
        "from datetime import datetime\n",
        "from google.oauth2 import service_account\n",
        "import mimetypes\n",
        "import os\n",
        "from google.cloud import storage\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import httplib2\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "from google.colab import sheets\n",
        "from IPython.display import HTML, display\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def read_df_from_sheet_gspread(sheet_url, worksheet_name=None):\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "\n",
        "    # Extract the sheet ID from the URL\n",
        "    sheet_id = sheet_url.split('/d/')[1].split('/')[0]\n",
        "\n",
        "    sheet = gc.open_by_key(sheet_id)\n",
        "    if worksheet_name:\n",
        "        worksheet = sheet.worksheet(worksheet_name)\n",
        "        values = worksheet.get_all_values()\n",
        "        df = pd.DataFrame(values[1:], columns=values[0])  # This assumes the first row is the header\n",
        "        return df\n",
        "\n",
        "    if worksheet_name is None and len(sheet.worksheets())==1:\n",
        "        worksheet = sheet.worksheets()[0]\n",
        "        values = worksheet.get_all_values()\n",
        "        df = pd.DataFrame(values[1:], columns=values[0])  # This assumes the first row is the header\n",
        "        return df\n",
        "\n",
        "    df_dict = {}\n",
        "    for worksheet in sheet.worksheets():\n",
        "    # Use the first sheet by default, or specify the name of the sheet you want to access\n",
        "\n",
        "        # Get all values from the sheet\n",
        "        values = worksheet.get_all_values()\n",
        "\n",
        "        # Convert to a pandas DataFrame\n",
        "        df = pd.DataFrame(values[1:], columns=values[0])  # This assumes the first row is the header\n",
        "        df_dict[worksheet.title] = df\n",
        "    return df_dict\n",
        "\n",
        "\n",
        "def post_to_rest_api(payload, url):\n",
        "    headers = {'Content-type': 'application/json', 'Accept': 'application/json'}\n",
        "    r = requests.post(url,\n",
        "                      json.dumps(payload),\n",
        "                      headers=headers)\n",
        "    return r\n",
        "\n",
        "def get_df_from_sheet(key, sheet_name):\n",
        "    url = 'http://decisionsciencelab.com/api/v1.0/get_sheet_json'\n",
        "    payload = {'key':key,\n",
        "               'sheet_name':sheet_name}\n",
        "    r = post_to_rest_api(payload, url)\n",
        "    return pd.DataFrame(r.json())\n",
        "\n",
        "\n",
        "def get_mongodb_url():\n",
        "    userid = secrets['mongodb_user']\n",
        "    password = urllib.parse.quote_plus(secrets['mongodb_password'])\n",
        "    mongodb_uri = \"mongodb+srv://\" + userid + \":\" + password + \"@location-selection.vfmji.gcp.mongodb.net/location_selection?retryWrites=true&w=majority\"\n",
        "    return mongodb_uri\n",
        "\n",
        "\n",
        "def get_document_list_from_mongodb(db_name, collection_name):\n",
        "    client = MongoClient(get_mongodb_url(), retryWrites=False)\n",
        "    database = client[db_name]\n",
        "    list_records = [doc for doc in database[collection_name].find()]\n",
        "    return list_records\n",
        "\n",
        "\n",
        "def get_df_from_mongodb(db_name, collection_name):\n",
        "    list_records = get_document_list_from_mongodb(db_name, collection_name)\n",
        "    df = pd.DataFrame(list_records)\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_collection(db_name, collection_name):\n",
        "    client = MongoClient(get_mongodb_url(), retryWrites=False)\n",
        "    database = client[db_name]\n",
        "    return database[collection_name]\n",
        "\n",
        "\n",
        "def insert(db_name, collection_name, json_doc):\n",
        "    collection = get_collection(db_name, collection_name)\n",
        "    collection.insert_one(json_doc)\n",
        "\n",
        "def insert_many(db_name, collection_name, json_docs):\n",
        "    \"\"\"Inserts multiple JSON documents into a MongoDB collection.\n",
        "\n",
        "    Args:\n",
        "        db_name: The name of the database.\n",
        "        collection_name: The name of the collection.\n",
        "        json_docs: A list of JSON documents to insert.\n",
        "    \"\"\"\n",
        "    collection = get_collection(db_name, collection_name)\n",
        "    collection.insert_many(json_docs)\n",
        "\n",
        "\n",
        "def upsert(db_name, collection_name, query, doc_to_upsert):\n",
        "    collection = get_collection(db_name, collection_name)\n",
        "    # collection.update(query, doc_to_upsert, upsert=True, safe=True)\n",
        "    collection.update_one(query, {'$set': doc_to_upsert}, upsert=True)\n",
        "\n",
        "def upsert_many(db_name, collection_name, json_docs, filter_key='_id'):\n",
        "    \"\"\"Upserts multiple JSON documents into a MongoDB collection.\n",
        "\n",
        "    Args:\n",
        "        db_name: The name of the database.\n",
        "        collection_name: The name of the collection.\n",
        "        json_docs: A list of JSON documents to upsert.\n",
        "        filter_key: The key to use for filtering existing documents.\n",
        "                    Defaults to '_id'.\n",
        "    \"\"\"\n",
        "    collection = get_collection(db_name, collection_name)\n",
        "    requests = []\n",
        "    for doc in json_docs:\n",
        "        filter = {filter_key: doc[filter_key]} if filter_key in doc else doc\n",
        "        update = {'$set': doc}\n",
        "        request = UpdateOne(filter, update, upsert=True)\n",
        "        requests.append(request)\n",
        "\n",
        "    if requests:\n",
        "        result = collection.bulk_write(requests)\n",
        "        print(f\"Upserted {result.upserted_count} documents, \"\n",
        "              f\"modified {result.modified_count} documents.\")\n",
        "\n",
        "\n",
        "def now():\n",
        "    return datetime.now()\n",
        "\n",
        "\n",
        "def get_gcp_bucket_credentials():\n",
        "    creds = {\n",
        "    \"type\": \"service_account\",\n",
        "    \"project_id\": \"cem-k8-test\",\n",
        "    \"private_key_id\": \"\",\n",
        "    \"private_key\": \"\",\n",
        "    \"client_email\": \"dslab-gcp-bucket@cem-k8-test.iam.gserviceaccount.com\",\n",
        "    \"client_id\": \"101834349465593903398\",\n",
        "    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "    \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "    \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/dslab-gcp-bucket%40cem-k8-test.iam.gserviceaccount.com\"\n",
        "    }\n",
        "    creds[\"private_key_id\"]=secrets['gcp_private_key_id']\n",
        "    creds[\"private_key\"]=secrets['gcp_private_key'].replace('\\\\n', '\\n')\n",
        "    gcp_bucket_credentials = service_account.Credentials.from_service_account_info(creds)\n",
        "    return gcp_bucket_credentials\n",
        "\n",
        "\n",
        "def upload_file_to_gcp_generic_mime_type(file_name, sub_folder_path, delete_file_from_local = False):\n",
        "    bucket_name = 'decision-science-lab-bucket'\n",
        "    project_id = 'cem-k8-test'\n",
        "    client = storage.Client(project=project_id, credentials=get_gcp_bucket_credentials())\n",
        "\n",
        "    mime_type, _ = mimetypes.guess_type(file_name)\n",
        "    if mime_type is None:\n",
        "        mime_type = 'application/octet-stream'  # Default/fallback MIME type\n",
        "    upload_file_name = file_name.replace(' ', '_')\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob = bucket.blob(f\"{sub_folder_path}/{upload_file_name}\")\n",
        "    blob.upload_from_filename(file_name, content_type=mime_type)\n",
        "\n",
        "    # Delete local file\n",
        "    if delete_file_from_local and os.path.exists(file_name):\n",
        "        os.remove(file_name)\n",
        "        print(f\"{file_name} successfully uploaded to GCP and deleted from local\")\n",
        "\n",
        "    uploaded_file_gcp_link = f'https://storage.googleapis.com/decision-science-lab-bucket/' \\\n",
        "                     f'{sub_folder_path}/{upload_file_name}'\n",
        "    print(uploaded_file_gcp_link)\n",
        "    return uploaded_file_gcp_link\n",
        "\n",
        "\n",
        "def get_all_urls_in_gcp_sub_folder_path(sub_folder_path):\n",
        "    bucket_name = 'decision-science-lab-bucket'\n",
        "    project_id = 'cem-k8-test'\n",
        "    client = storage.Client(project=project_id, credentials=get_gcp_bucket_credentials())\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blobs = client.list_blobs(bucket_name, prefix=sub_folder_path + '/')\n",
        "    url_prefix = 'https://storage.googleapis.com/decision-science-lab-bucket/'\n",
        "    url_list = []\n",
        "    for blob in blobs:\n",
        "        url_list.append(url_prefix+blob.name)\n",
        "    return url_list\n",
        "\n",
        "\n",
        "def get_secrets():\n",
        "    sheet_url = 'https://docs.google.com/spreadsheets/d/1mLwdiSnTi0KoB8Zg6kMclTXXm3f_JavMX-5lAUp-Ry0/edit#gid=0'\n",
        "    sheet_name = 'dev'\n",
        "    df_keys = read_df_from_sheet_gspread(sheet_url, worksheet_name=sheet_name)\n",
        "    key_dict = dict(zip(df_keys['key'], df_keys['value']))\n",
        "    return key_dict\n",
        "\n",
        "\n",
        "secrets = get_secrets()"
      ],
      "metadata": {
        "id": "9gSvmGJK4PPb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "def upload_gdrive_file_to_gcp_generic_mime_type(file_path, gcp_path):\n",
        "    video_filename = file_path.split('/')[-1]\n",
        "    destination_path = f'/content/{video_filename}' # Copy to current Colab directory\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        shutil.copy2(file_path, destination_path) # copy2 preserves metadata\n",
        "        print(f\"File '{file_path}' copied successfully to '{destination_path}'\")\n",
        "    else:\n",
        "        print(f\"Error: Source file '{file_path}' not found.\")\n",
        "    video_url = upload_file_to_gcp_generic_mime_type(video_filename, gcp_path, delete_file_from_local = True)\n",
        "    return video_url\n",
        "\n",
        "\n",
        "# sub_folder_path = 'liplips/roma_silvers_products'\n",
        "# urls = get_all_urls_in_gcp_sub_folder_path(sub_folder_path)\n",
        "# urls[:5]\n",
        "# len(urls)"
      ],
      "metadata": {
        "id": "HmubhYeW5ThV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_name_for_export_with_date_time(file_name_header, file_extenstion):\n",
        "    formatted_datetime = now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
        "    file_name = f\"{file_name_header}_{formatted_datetime}.{file_extenstion}\"\n",
        "    file_name = file_name.replace(' ', '_')\n",
        "    return file_name"
      ],
      "metadata": {
        "id": "RMMnAdMy8Rfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_time_stamp_string():\n",
        "    return now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
        "\n",
        "# get_time_stamp_string()"
      ],
      "metadata": {
        "id": "C5zFM_6z8uSo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fcdeacf3-930c-42d6-8bbb-a6c76e1af729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'07_27_2024_11_59_45'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import json_normalize\n",
        "\n",
        "def sum_last_3_views(performance_metrics, field):\n",
        "    \"\"\"Sums the views from the last 3 metric records.\n",
        "\n",
        "    Args:\n",
        "        performance_metrics: A list of performance metrics.\n",
        "\n",
        "    Returns:\n",
        "        The sum of views from the last 3 records, or 0 if there are fewer than 3 records.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        last_3_records = performance_metrics[-3:]\n",
        "        total_views = sum([record.get(field, 0) for record in last_3_records])\n",
        "        return total_views\n",
        "    except (TypeError, IndexError):\n",
        "        return 0  # Handle cases with missing data or fewer than 3 records\n",
        "\n",
        "def get_start_end_dates(performance_metrics):\n",
        "    \"\"\"Gets the start date of the -3 record and end date of the last record.\n",
        "\n",
        "    Args:\n",
        "        performance_metrics: A list of performance metrics.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the start and end dates, or (None, None) if there are issues.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        length = len(performance_metrics)\n",
        "        start_date = performance_metrics[-min(3, length)]['start_date']  # Assuming 'start_date' is the key\n",
        "        end_date = performance_metrics[-1]['end_date']   # Assuming 'end_date' is the key\n",
        "        return pd.to_datetime(start_date), pd.to_datetime(end_date)\n",
        "    except (TypeError, IndexError, KeyError):\n",
        "        return None, None  # Handle potential errors\n",
        "\n",
        "def get_tag_list(df_row):\n",
        "    tag_list = []\n",
        "    for i in range(1,14):\n",
        "        if pd.notnull(df_row[f'tag_{i}']) and len(df_row[f'tag_{i}'])>0:\n",
        "            tag_list.append(df_row[f'tag_{i}'])\n",
        "    # print(tag_list)\n",
        "    return tag_list\n",
        "\n",
        "def get_current_etsy_data_df():\n",
        "    etsy_all_data_df = get_df_from_mongodb('location_selection', 'etsy_listings')\n",
        "    etsy_all_data_df['performance_metrics_last_read'] = etsy_all_data_df['performance_metrics'].apply(lambda x: x[-1] if type(x)==list else None)\n",
        "    columns = ['views', 'favourites', 'order', 'revenue']\n",
        "    for column in columns:\n",
        "        etsy_all_data_df[f'total_{column}_last_3'] = etsy_all_data_df['performance_metrics'].apply(sum_last_3_views, field=column)\n",
        "    etsy_all_data_df[['start_date', 'end_date']] = etsy_all_data_df['performance_metrics'].apply(lambda x: pd.Series(get_start_end_dates(x)))\n",
        "    etsy_all_data_df['start_date'] = pd.to_datetime(etsy_all_data_df['start_date'])\n",
        "    etsy_all_data_df['end_date'] = pd.to_datetime(etsy_all_data_df['end_date'])\n",
        "    etsy_all_data_df['metric_days'] = (etsy_all_data_df['end_date'] - etsy_all_data_df['start_date']).dt.days\n",
        "    etsy_all_data_df['tag_list'] = etsy_all_data_df.apply(get_tag_list, axis=1)\n",
        "    return etsy_all_data_df\n",
        "\n",
        "# df = get_current_etsy_data_df()\n",
        "# df[['listing_id', 'total_views_last_3',\n",
        "#                   'total_favourites_last_3','total_revenue_last_3',\n",
        "#                   'total_order_last_3',\n",
        "#                   'start_date', 'end_date', 'metric_days' ]]"
      ],
      "metadata": {
        "id": "09r2DE8sDndq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "66gR5maSGzd3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U54iFjARbmux",
        "outputId": "bb329dda-43ae-4a59-9d5f-e9b434a08108"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['labradorite pendant', 'labradorite necklace', 'unique necklace', 'unique pendant', 'moonstone pendant', 'moonstone necklace', 'star necklace', 'moon necklace', 'moon stone necklace', 'unique jewelry', 'unique jewellery', 'gemstone pendant']\n",
            "['silver bird pendant', 'silver bird necklace', 'minimalist necklace', 'minimalist pendant', 'unique necklace', 'bird necklace', 'bird necklace silver', 'bird pendant', 'bird pendant silver', 'bird silver necklace', 'pendant bird', 'silver necklace bird']\n",
            "['apatite', 'gem stone', 'apetite', 'apatite necklace', 'gemstone necklace', 'bird necklace', 'seagull necklace', 'blue beaded necklace', 'beaded necklace', '925 sterling silver', 'silver necklace', 'apatite gemstone']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQ3ep06xcBWK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}